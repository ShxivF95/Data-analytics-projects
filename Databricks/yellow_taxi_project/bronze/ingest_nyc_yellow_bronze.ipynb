{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ad2d33f-6161-410a-8d5b-55ed1d0e27a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Auto Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "610f9fd7-ca71-4104-a7c8-a36895c4e9f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Databricks needs permission to read our files from ADLS. so, we have to create an External Location in the Databricks Catalog.**\n",
    "- **Unity Catalog enables centralized governance, fine-grained access control, auditing, and secure access across workspaces. It is mandatory for enterprise-grade security, compliance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838eb910-96dc-4478-a7da-87387ed3226f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW EXTERNAL LOCATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db296937-2810-49d3-a44a-6b7fad28be82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**Databricks Auto Loader incrementally ingests new files from cloud storage with exactly-once guarantees at the file level using checkpointing, supports schema evolution, and is typically used for raw ingestion in a medallion architecture.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a301c6f-7eb9-4455-9220-c3f1c0f6a755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Auto Loader detects only new files added to a directory and processes them exactly once.**\n",
    "- **It achieves idempotency at the file level using checkpointing and file metadata.**\n",
    "- **Auto Loader tracks files, not rows.**\n",
    "- **It supports schema inference and schema evolution, allowing new columns to be added safely.**\n",
    "- **Auto Loader does not perform upserts; it only appends data. Upserts are implemented later using Delta Lake MERGE logic in silver layer**\n",
    "\n",
    "- **The job must run (continuous or triggered); Auto Loader itself does not run automatically.**\n",
    "- **Auto Loader is NOT allowed to read from ADLS directly on shared clusters when Unity Catalog permission enforcement is enabled. that means we must register the external location for it in unity catalog.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228cbedf-b9a8-4d7d-9b44-de62465282c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"abfss://nycsync@hisadls.dfs.core.windows.net/checkpoint\"\n",
    "file_path = \"abfss://landing@hisadls.dfs.core.windows.net/Nyc_taxi\"\n",
    "\n",
    "\n",
    "df = spark.readStream.format(\"cloudFiles\")\\\n",
    "  .option(\"cloudFiles.format\", \"parquet\")\\\n",
    "    .option(\"cloudFiles.schemaLocation\", checkpoint_path)\\\n",
    "      .option(\"cloudFiles.inferSchema\", True)\\\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\\\n",
    "          .load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44635460-9c31-4c8a-8156-5b7c15a18f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "brf = df.withColumn(\"input_filename\", col(\"_metadata.file_path\"))\\\n",
    "    .withColumn(\"ingested_at\", current_timestamp())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2468d6c4-b0ce-42fb-9be0-7604f2c27507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "brf.writeStream.format(\"delta\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "        .trigger(once=True)\\\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\\\n",
    "            .toTable(\"nyc_yellow.bronzee.nyc_yellow\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23e755c3-3d9d-41d4-8c84-5537ff6301af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Here iâ€™m using Auto Loader with trigger(once) to get the benefits of incremental file processing and checkpointing, while still running it as a batch job. This makes the pipeline future-proof if ingestion frequency increases."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8541779409328750,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_nyc_yellow_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
